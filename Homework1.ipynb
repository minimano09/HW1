{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97c1b89",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "You are to implement the stages of finding textually similar documents based on Jaccard similarity using the shingling, minhashing, and locality-sensitive hashing (LSH) techniques and corresponding algorithms. The implementation can be done using any big data processing framework, such as Apache Spark, Apache Flink, or no framework, e.g., in Java, Python, etc. To test and evaluate your implementation, write a program that uses your implementation to find similar documents in a corpus of 5-10 or more documents, such as web pages or emails.\n",
    "\n",
    "The stages should be implemented as a collection of classes, modules, functions, or procedures depending on the framework and the language of your choice. Below, we describe sample classes implementing different stages of finding textually similar documents. You do not have to develop the exact same classes and data types described below. Feel free to use data structures that suit you best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6926df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import hashlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae8a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/07 17:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# initializing Spark\n",
    "findspark.init()\n",
    "conf = SparkConf().setAppName(\"SimDoc\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204a54f",
   "metadata": {},
   "source": [
    "### Pre-process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228524bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts from dataframes\n",
    "df = pd.read_csv(\"datasets/custom_job_dataset.csv\", delimiter=',')\n",
    "\n",
    "docus = df['description'].dropna().tolist()\n",
    "\n",
    "# creating a list of documents where we store all our texts\n",
    "documents_map = {i: content for i, content in enumerate(docus)}\n",
    "documents_map = dict(list(documents_map.items())[:10])\n",
    "\n",
    "# testing text creation\n",
    "#print(len(documents_map))\n",
    "#print(documents_map[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27058d0f",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cb4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  #Shingle length\n",
    "signature_len = 100  #Length of minhash signatures\n",
    "similarity_threshold = 0.8  #Similarity threshold\n",
    "b = 20  #Number of bands in LSH\n",
    "r = signature_len // b #Rows per band in LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d17ce0",
   "metadata": {},
   "source": [
    "### Shingling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397cd5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles(id, text, k):\n",
    "    \"\"\"Creating k-shingles from a given text\n",
    "\n",
    "    Args:\n",
    "        id: ID of the document\n",
    "        text: Given document\n",
    "        k: Length of the shingles\n",
    "\n",
    "    Returns:\n",
    "        ID of the document with the created shingles\n",
    "    \"\"\"\n",
    "    content = text.lower().replace('.', '').replace(',', '')\n",
    "    shingles = set(text[i:i + k] for i in range(len(text) - k + 1))\n",
    "    return id, shingles\n",
    "\n",
    "\n",
    "def generate_hashed_shingles(text):\n",
    "    \"\"\"Generate hashed shingles for each document\n",
    "    \n",
    "    Args:\n",
    "        text: Shingles of a given document\n",
    "\n",
    "    Returns:\n",
    "        ID of the document and set of the hashed shingles\n",
    "    \"\"\"\n",
    "    id, shingles = text\n",
    "    hashed_shingles = set(hash(shingle) for shingle in shingles)\n",
    "    return id, hashed_shingles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6471ee",
   "metadata": {},
   "source": [
    "### Jaccard Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(docu1, docu2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets of shingles\n",
    "    \n",
    "    Args:\n",
    "        docu1: First document for comparison \n",
    "        docu2: Second document for comparison\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    id1, shingles1 = docu1\n",
    "    id2, shingles2 = docu2\n",
    "    intersection = shingles1.intersection(shingles2)\n",
    "    union = shingles1.union(shingles2)\n",
    "    similarity = len(intersection) / len(union) if union else 0.0\n",
    "    return (id1, id2), similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e25fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents_map to an RDD\n",
    "jaccard_rdd = sc.parallelize(documents_map.items())\n",
    "\n",
    "# create shinglings\n",
    "shingles_rdd = jaccard_rdd.map(lambda doc: create_shingles(doc[0], doc[1], k))\n",
    "hashed_shingles_rdd = shingles_rdd.map(generate_hashed_shingles)\n",
    "\n",
    "# getting all pairs\n",
    "jaccard_pairs = hashed_shingles_rdd.cartesian(hashed_shingles_rdd).filter(lambda x: x[0][0] < x[1][0]) #remove duplicated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f79d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing Jaccard similarities \n",
    "jaccard_pairs_with_similarities = jaccard_pairs.map(lambda pair: jaccard_similarity(pair[0], pair[1]))\n",
    "jaccard_pairs_threshold = jaccard_pairs_with_similarities.filter(lambda x: x[1] >= similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4329d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((4, 5), 0.9680111265646731)\n",
      "((4, 8), 0.9789915966386554)\n",
      "((5, 8), 0.9667128987517337)\n"
     ]
    }
   ],
   "source": [
    "# print out the pairs which are similar enough based on the threshold\n",
    "result = jaccard_pairs_threshold.collect()\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203095e9",
   "metadata": {},
   "source": [
    "### MinHash\n",
    "\n",
    "- source of next_prime: http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
    "- minHash source: https://github.com/chrisjmccormick/MinHash/blob/master/runMinHashExample.py\n",
    "- hash function: h(x) = (a*x + b) % max_shingle_id\n",
    "    - a, b: random coefficients - these are fixed we generate it ones\n",
    "    - always choose the minimum from the hashed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770f83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_shingle_id = 2**32-1\n",
    "\n",
    "#  h(x) = (a*x + b) % max_shingle_id\n",
    "# a, b: random coefficients\n",
    "\n",
    "def get_coefficients():\n",
    "    #max_shingle_id: it can be any integer number based on the range for the coeffs\n",
    "    coeffs = []\n",
    "    while len(coeffs) < signature_len:\n",
    "        rand_idx = random.randint(1, max_shingle_id) \n",
    "        #print('anyad')\n",
    "        while rand_idx in coeffs:\n",
    "            rand_idx = random.randint(1, max_shingle_id)\n",
    "            #print('apad')\n",
    "        coeffs.append(rand_idx)\n",
    "    return coeffs\n",
    "\n",
    "coeffs_a = get_coefficients()\n",
    "coeffs_b = get_coefficients()\n",
    "    \n",
    "def get_minhash_signature(hashed_shingles):\n",
    "    signature = []\n",
    "    for i in range(signature_len):\n",
    "        min_hash_code = min([(coeffs_a[i] * shingle + coeffs_b[i]) % max_shingle_id for shingle in hashed_shingles])\n",
    "        signature.append(min_hash_code)\n",
    "    return signature\n",
    "\n",
    "def minhash_docu(id, hashed_shingles):\n",
    "    return id, get_minhash_signature(hashed_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03dadeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_similarity(docu1, docu2, signature_len):\n",
    "    id1, signature1 = docu1\n",
    "    id2, signature2 = docu2\n",
    "    \n",
    "    agree_cnt = sum(1 for i in range(signature_len) if signature1[i] == signature2[i])\n",
    "    similarity = agree_cnt / signature_len\n",
    "    \n",
    "    return (id1, id2), similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b1cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash_rdd = hashed_shingles_rdd.map(lambda doc: minhash_docu(doc[0], doc[1]))\n",
    "minhash_pairs = minhash_rdd.cartesian(minhash_rdd).filter(lambda x: x[0][0] < x[1][0]) # remove duplicates\n",
    "minhash_with_similarities = minhash_pairs.map(lambda doc: signature_similarity(doc[0], doc[1], signature_len))\n",
    "minhash_threshold = minhash_with_similarities.filter(lambda x: x[1] >= similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36d42b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((4, 5), 0.98)\n",
      "((4, 8), 0.98)\n",
      "((5, 8), 0.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = minhash_threshold.collect()\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7de937",
   "metadata": {},
   "source": [
    "### LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8cfe832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bands_by_signature(doc, b, r):\n",
    "    \"\"\"Split signature vectors into bands\n",
    "\n",
    "    Args:\n",
    "        doc : It's a pair ina  following format (docu_id, signature)\n",
    "        b : Number of bands\n",
    "        r: Number of row in each band\n",
    "\n",
    "    Returns:\n",
    "        A list where every element is a ((band_id, band_hash), docu_id)\n",
    "    \"\"\"\n",
    "    docu_id, signature = doc\n",
    "    bands = []\n",
    "    for i in range(b):\n",
    "        start_index = i * r\n",
    "        end_index = start_index + r\n",
    "        band = tuple(signature[start_index:end_index])  #Convert to tuple for consistency in hashing\n",
    "        bands.append(((i, hash(band)), docu_id))  #Use (band_id, band_hash) as the key\n",
    "    return bands\n",
    "\n",
    "\n",
    "def generate_candidate_pairs(docu):\n",
    "    \"\"\"Generating candidate pairs from buckets and avoid duplicates\n",
    "\n",
    "    Args:\n",
    "        docu: ((band_id, band_hash), [docu_ids]), where the pair first element is a pair containing the id and the hash of the given band \n",
    "            and the pair second element is a list of document ids whose hashed value in the same band are the same\n",
    "\n",
    "    Returns:\n",
    "        Candidate pairs where every element is a (docu_id1, docu_id2)\n",
    "    \"\"\"\n",
    "    _, docu_ids = docu\n",
    "    candidates = []\n",
    "    if len(docu_ids) > 1:\n",
    "        for i in range(len(docu_ids)):\n",
    "            for j in range(i + 1, len(docu_ids)):\n",
    "                candidates.append((docu_ids[i], docu_ids[j]))\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def compute_signature_similarity(docus_with_signitures):\n",
    "    \"\"\"Computing similarities between 2 documents in the same band\n",
    "\n",
    "    Args:\n",
    "        docus_with_signitures: Getting te data in the following format: ((docu_id1, docu_id2), [(docu_id1, signature), (docu_id2, signature)])\n",
    "\n",
    "    Returns:\n",
    "        Document is pairs with the similarity between the 2 documents\n",
    "    \"\"\"\n",
    "    (docu_id1, docu_id2), docs_signatures = docus_with_signitures\n",
    "    sig_dict = {doc_id: signature for doc_id, signature in docs_signatures}\n",
    "    if docu_id1 in sig_dict and docu_id2 in sig_dict:\n",
    "        signature1 = sig_dict[docu_id1]\n",
    "        signature2 = sig_dict[docu_id2]\n",
    "        # Calculate the similarity based on matching elements in the signatures\n",
    "        agree_cnt = sum(1 for i in range(signature_len) if signature1[i] == signature2[i])\n",
    "        similarity = agree_cnt / signature_len\n",
    "        return ((docu_id1, docu_id2), similarity)\n",
    "    else:\n",
    "        # One of the signatures is missing; return zero similarity\n",
    "        return ((docu_id1, docu_id2), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cb85177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contains ((band_id, band_hash), docu_id) pairs\n",
    "band_buckets_rdd = minhash_rdd.flatMap(lambda doc: create_bands_by_signature(doc, b, r))\n",
    "\n",
    "#Group documents by (band_id, band_hash) to get lists of doc_ids in the same bucket so we get the following format ((band_id, band_hash), [docu_ids])\n",
    "bucketed_docus_rdd = band_buckets_rdd.groupByKey().mapValues(list)\n",
    "# Generating candidate pairs from buckets with more than one document in the format os (docu_id1, docu_id2)\n",
    "candidate_pairs_rdd = bucketed_docus_rdd.flatMap(lambda docu: generate_candidate_pairs(docu)).distinct()\n",
    "# ((docu_id1, (docu_id1, docu_id2)), (docu_id2, (docu_id1, docu_id2))) to make join with the minhash_rdd\n",
    "candidate_pairs_with_keys_rdd = candidate_pairs_rdd.flatMap(lambda pair: [(pair[0], pair), (pair[1], pair)])\n",
    "\n",
    "# Attaching each document's signature to its part of the candidate pair\n",
    "# Joined the candidate_pairs_with_keys with minhash_rdd and get the following format: (docu_id, ((docu_id1, docu_id2), signature))\n",
    "joined_rdd = candidate_pairs_with_keys_rdd.join(minhash_rdd)\n",
    "\n",
    "# ((docu_id1, docu_id2), (docu_id, signature)) where key is the pair and we generate 2 result where docu_id=docu_id1 and in the other one docu_id=docu_id2\n",
    "signatures_by_pair_rdd = joined_rdd.map(lambda x: (x[1][0], (x[0], x[1][1])))\n",
    "\n",
    "# Grouping by candidate pair to collect both signatures in the following format ((docu_id1, docu_id2), [(docu_id1, signature), (docu_id2, dignature)])\n",
    "grouped_signatures_rdd = signatures_by_pair_rdd.groupByKey().mapValues(list)\n",
    "\n",
    "# Calculating similarities and get the following format: ((docu_id1, docu_id2), similarity)\n",
    "similarities_rdd = grouped_signatures_rdd.map(lambda pair: compute_signature_similarity(pair))\n",
    "# Filtering with the given threshold\n",
    "filtered_similar_docs_rdd = similarities_rdd.filter(lambda x: x[1] >= similarity_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0812ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((4, 5), 0.98)\n",
      "((4, 8), 0.98)\n",
      "((5, 8), 0.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "result = filtered_similar_docs_rdd.collect()\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a3940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
