{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97c1b89",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "You are to implement the stages of finding textually similar documents based on Jaccard similarity using the shingling, minhashing, and locality-sensitive hashing (LSH) techniques and corresponding algorithms. The implementation can be done using any big data processing framework, such as Apache Spark, Apache Flink, or no framework, e.g., in Java, Python, etc. To test and evaluate your implementation, write a program that uses your implementation to find similar documents in a corpus of 5-10 or more documents, such as web pages or emails.\n",
    "\n",
    "The stages should be implemented as a collection of classes, modules, functions, or procedures depending on the framework and the language of your choice. Below, we describe sample classes implementing different stages of finding textually similar documents. You do not have to develop the exact same classes and data types described below. Feel free to use data structures that suit you best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6926df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import hashlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae8a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 20:31:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# initializing Spark\n",
    "findspark.init()\n",
    "conf = SparkConf().setAppName(\"SimDoc\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228524bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts from dataframes\n",
    "df1 = pd.read_csv(\"datasets/winemag-data_first150k.csv\")\n",
    "\n",
    "docus = df1['description'].dropna().tolist()\n",
    "\n",
    "# creating a list of documents where we store all our texts\n",
    "documents_map = {i: content for i, content in enumerate(docus)}\n",
    "documents_map = dict(list(documents_map.items())[:10])\n",
    "\n",
    "# testing text creation\n",
    "#print(len(documents_map))\n",
    "#print(documents_map[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cb4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 5  # Shingle length\n",
    "signature_len = 100  # Length of minhash signatures\n",
    "similarity_threshold = 0.1  # Similarity threshold for LSH\n",
    "num_bands = 20  # Number of bands in LSH\n",
    "rows_per_band = signature_len // num_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397cd5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles(id, text, k):\n",
    "    \"\"\"Create k-shingles for a document.\"\"\"\n",
    "    content = text.lower().replace('.', '').replace(',', '')\n",
    "    shingles = set(text[i:i + k] for i in range(len(text) - k + 1))\n",
    "    return id, shingles\n",
    "\n",
    "\n",
    "def generate_hashed_shingles(text):\n",
    "    \"\"\"Generate hashed shingles for each document.\"\"\"\n",
    "    id, shingles = text\n",
    "    hashed_shingles = set(hash(shingle) for shingle in shingles)\n",
    "    return id, hashed_shingles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6471ee",
   "metadata": {},
   "source": [
    "### Jaccard Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(docu1, docu2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets of shingles.\"\"\"\n",
    "    id1, shingles1 = docu1\n",
    "    id2, shingles2 = docu2\n",
    "    intersection = shingles1.intersection(shingles2)\n",
    "    union = shingles1.union(shingles2)\n",
    "    similarity = len(intersection) / len(union) if union else 0.0\n",
    "    return (id1, id2), similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e25fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents_map to an RDD\n",
    "jaccard_rdd = sc.parallelize(documents_map.items())\n",
    "\n",
    "# create shinglings\n",
    "shingles_rdd = jaccard_rdd.map(lambda doc: create_shingles(doc[0], doc[1], k))\n",
    "hashed_shingles_rdd = shingles_rdd.map(generate_hashed_shingles)\n",
    "\n",
    "# getting all pairs\n",
    "jaccard_pairs = hashed_shingles_rdd.cartesian(hashed_shingles_rdd).filter(lambda x: x[0][0] < x[1][0]) #remove duplicated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f79d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing Jaccard similarities \n",
    "jaccard_pairs_with_similarities = jaccard_pairs.map(lambda pair: jaccard_similarity(pair[0], pair[1]))\n",
    "jaccard_pairs_threshold = jaccard_pairs_with_similarities.filter(lambda x: x[1] >= similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4329d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:==========================================>              (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 3), 0.10535117056856187)\n",
      "((1, 5), 0.166015625)\n",
      "((1, 7), 0.15412844036697249)\n",
      "((3, 5), 0.11260504201680673)\n",
      "((5, 6), 0.11070110701107011)\n",
      "((5, 7), 0.12099644128113879)\n",
      "((6, 7), 0.11228070175438597)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print out the pairs which are not below the threshold and similar enough\n",
    "result = jaccard_pairs_threshold.collect()\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203095e9",
   "metadata": {},
   "source": [
    "### MinHash\n",
    "\n",
    "- source of next_prime: http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
    "- minHash source: https://github.com/chrisjmccormick/MinHash/blob/master/runMinHashExample.py\n",
    "- hash function: h(x) = (a*x + b) % max_shingle_id\n",
    "    - a, b: random coefficients - these are fixed we generate it ones\n",
    "    - always choose the minimum from the hashed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f83bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3864713615, 478001295, 301531630, 3216443062, 4049061597, 41551255, 2736980200, 2942452256, 1287632781, 2314594600, 408633167, 841068115, 773500145, 3923002983, 2220638513, 118087531, 526814677, 8603579, 4211948036, 2722277260, 3495604815, 2113013151, 572458016, 1010439377, 104103746, 2242861726, 3749222777, 366216041, 1867447115, 2841861178, 3872926141, 145562691, 1894556135, 2251729982, 4114990643, 2374578010, 2380349444, 2050083027, 124722449, 3824550241, 2312688337, 953128213, 1540181870, 3365116640, 1015748145, 742748107, 3138377816, 3248356770, 113493578, 3343118406, 1077007330, 554669489, 362227268, 4211159107, 3936370102, 953852285, 1041396583, 1239194279, 1879396790, 1771097148, 733740683, 4211150561, 71733968, 1645338532, 3979405313, 564874982, 2100351697, 227898626, 2711341495, 3798481561, 3303384349, 3696190304, 4124073711, 64718181, 2092744766, 1583367596, 655165754, 2638127525, 2135666295, 4263558313, 3510820552, 1024910116, 2619700470, 2208567758, 2498155952, 2766418922, 1032667360, 3532001526, 4005580388, 4240176103, 1286746949, 3812502978, 855360567, 1846533344, 320468896, 3524610820, 2273887963, 157440827, 463354127, 3054094529]\n",
      "[3257371749, 396711884, 1523880925, 1076332150, 1587159275, 3819745012, 794088248, 465863641, 3894964799, 2415909290, 2840418506, 662246768, 4107376951, 3508975838, 1410035011, 3903233544, 2771859287, 1230154421, 1151362284, 2760412588, 834213859, 2899129555, 491343923, 88755774, 158471025, 797078610, 2937280526, 55853507, 728501250, 2847657455, 4128785026, 1223279119, 776536672, 322493224, 2584438805, 2144762081, 3498238126, 4043233748, 2967929047, 9207461, 2639790749, 1458323809, 462178571, 1095029054, 2340947715, 3918349035, 140706731, 2184612682, 1932846450, 2667750029, 3451888987, 2064439998, 1897205459, 2449419542, 4082310054, 2992717448, 3125570679, 3849709325, 2429822262, 1608867739, 362515700, 3236158001, 3495240076, 4097954774, 1677119597, 1124516967, 2905980126, 1002084450, 3398840156, 609342851, 2216130985, 2263836075, 3045835130, 2558441528, 3745822391, 1946264813, 2303260827, 2106694001, 2488169485, 2227911233, 2688025604, 2723764521, 531238812, 966306286, 3696995532, 4214810924, 1970845006, 752284015, 597172677, 3241171861, 1475286926, 2030058131, 1266654625, 2341484534, 3982629387, 3369608819, 541624000, 2527880857, 1137604983, 3418275713]\n"
     ]
    }
   ],
   "source": [
    "#signature_len = 100 \n",
    "max_shingle_id = 2**32-1\n",
    "\n",
    "#  h(x) = (a*x + b) % max_shingle_id\n",
    "# a, b: random coefficients\n",
    "\n",
    "def get_coefficients():\n",
    "    #max_shingle_id: it can be any integer number based on the range for the coeffs\n",
    "    coeffs = []\n",
    "    while len(coeffs) < signature_len:\n",
    "        rand_idx = random.randint(1, max_shingle_id) \n",
    "        #print('anyad')\n",
    "        while rand_idx in coeffs:\n",
    "            rand_idx = random.randint(1, max_shingle_id)\n",
    "            #print('apad')\n",
    "        coeffs.append(rand_idx)\n",
    "    return coeffs\n",
    "\n",
    "coeffs_a = get_coefficients()\n",
    "coeffs_b = get_coefficients()\n",
    "    \n",
    "def get_minhash_signature(hashed_shingles):\n",
    "    signature = []\n",
    "    for i in range(signature_len):\n",
    "        min_hash_code = min([(coeffs_a[i] * shingle + coeffs_b[i]) % max_shingle_id for shingle in hashed_shingles])\n",
    "        signature.append(min_hash_code)\n",
    "    return signature\n",
    "\n",
    "def minhash_docu(id, hashed_shingles):\n",
    "    return id, get_minhash_signature(hashed_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03dadeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_similarity(docu1, docu2, signature_len):\n",
    "    id1, signature1 = docu1\n",
    "    id2, signature2 = docu2\n",
    "    \n",
    "    agree_cnt = sum(1 for i in range(signature_len) if signature1[i] == signature2[i])\n",
    "    similarity = agree_cnt / signature_len\n",
    "    \n",
    "    return (id1, id2), similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2b1cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash_rdd = hashed_shingles_rdd.map(lambda doc: minhash_docu(doc[0], doc[1]))\n",
    "minhash_pairs = minhash_rdd.cartesian(minhash_rdd).filter(lambda x: x[0][0] < x[1][0]) # remove duplicates\n",
    "minhash_with_similarities = minhash_pairs.map(lambda doc: signature_similarity(doc[0], doc[1], signature_len))\n",
    "minhash_threshold = minhash_with_similarities.filter(lambda x: x[1] >= similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36d42b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 3), 0.12)\n",
      "((1, 5), 0.17)\n",
      "((1, 6), 0.12)\n",
      "((1, 7), 0.14)\n",
      "((1, 8), 0.13)\n",
      "((3, 5), 0.11)\n",
      "((3, 6), 0.1)\n",
      "((3, 8), 0.12)\n",
      "((4, 8), 0.12)\n",
      "((5, 6), 0.11)\n",
      "((5, 7), 0.16)\n",
      "((6, 7), 0.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = minhash_threshold.collect()\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb85177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
