{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97c1b89",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "You are to implement the stages of finding textually similar documents based on Jaccard similarity using the shingling, minhashing, and locality-sensitive hashing (LSH) techniques and corresponding algorithms. The implementation can be done using any big data processing framework, such as Apache Spark, Apache Flink, or no framework, e.g., in Java, Python, etc. To test and evaluate your implementation, write a program that uses your implementation to find similar documents in a corpus of 5-10 or more documents, such as web pages or emails.\n",
    "\n",
    "The stages should be implemented as a collection of classes, modules, functions, or procedures depending on the framework and the language of your choice. Below, we describe sample classes implementing different stages of finding textually similar documents. You do not have to develop the exact same classes and data types described below. Feel free to use data structures that suit you best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6926df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import hashlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae8a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/04 21:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/04 21:02:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# initializing Spark\n",
    "findspark.init()\n",
    "conf = SparkConf().setAppName(\"SimDoc\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228524bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts from dataframes\n",
    "df1 = pd.read_csv(\"datasets/winemag-data_first150k.csv\")\n",
    "\n",
    "docus = df1['description'].dropna().tolist()\n",
    "\n",
    "# creating a list of documents where we store all our texts\n",
    "documents_map = {i: content for i, content in enumerate(docus)}\n",
    "documents_map = dict(list(documents_map.items())[:20])\n",
    "\n",
    "# testing text creation\n",
    "#print(len(documents_map))\n",
    "#print(documents_map[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cb4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 5  # Shingle length\n",
    "signature_len = 100  # Length of minhash signatures\n",
    "similarity_threshold = 0.8  # Similarity threshold for LSH\n",
    "num_bands = 20  # Number of bands in LSH\n",
    "rows_per_band = signature_len // num_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397cd5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles(id, text, k):\n",
    "    \"\"\"Create k-shingles for a document.\"\"\"\n",
    "    content = text.lower().replace('.', '').replace(',', '')\n",
    "    shingles = set(text[i:i + k] for i in range(len(text) - k + 1))\n",
    "    return id, shingles\n",
    "\n",
    "\n",
    "def generate_hashed_shingles(text):\n",
    "    \"\"\"Generate hashed shingles for each document.\"\"\"\n",
    "    id, shingles = text\n",
    "    hashed_shingles = set(hash(shingle) for shingle in shingles)\n",
    "    return id, hashed_shingles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6471ee",
   "metadata": {},
   "source": [
    "### Jaccard Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e8360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(docu1, docu2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets of shingles.\"\"\"\n",
    "    id1, shingles1 = docu1\n",
    "    id2, shingles2 = docu2\n",
    "    intersection = shingles1.intersection(shingles2)\n",
    "    union = shingles1.union(shingles2)\n",
    "    similarity = len(intersection) / len(union) if union else 0.0\n",
    "    return (id1, id2), similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e25fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents_map to an RDD\n",
    "jaccard_rdd = sc.parallelize(documents_map.items())\n",
    "\n",
    "# create shinglings\n",
    "shingles_rdd = jaccard_rdd.map(lambda doc: create_shingles(doc[0], doc[1], k))\n",
    "hashed_shingles_rdd = shingles_rdd.map(generate_hashed_shingles)\n",
    "\n",
    "# getting all pairs\n",
    "jaccard_pairs = hashed_shingles_rdd.cartesian(hashed_shingles_rdd).filter(lambda x: x[0][0] < x[1][0]) #remove duplicated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f79d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing Jaccard similarities \n",
    "jaccard_pairs_with_similarities = jaccard_pairs.map(lambda pair: jaccard_similarity(pair[0], pair[1]))\n",
    "jaccard_pairs_threshold = jaccard_pairs_with_similarities.filter(lambda x: x[1] >= similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4329d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print out the pairs which are not below the threshold and similar enough\n",
    "result = jaccard_pairs_threshold.collect()\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203095e9",
   "metadata": {},
   "source": [
    "### MinHash\n",
    "\n",
    "- source of next_prime: http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
    "- minHash source: https://github.com/chrisjmccormick/MinHash/blob/master/runMinHashExample.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "770f83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#signature_len = 100\n",
    "next_prime = 4294967311 \n",
    "max_shingle_id = 2**32-1\n",
    "\n",
    "#  h(x) = (a*x + b) % max_shingle_id\n",
    "# a, b: random coefficients\n",
    "\n",
    "def get_coefficients():\n",
    "    coeffs = []\n",
    "    while len(coeffs) < signature_len:\n",
    "        rand_idx = random.randint(1, max_shingle_id) \n",
    "        while rand_idx in coeffs:\n",
    "            rand_idx = random.randint(1, max_shingle_id)\n",
    "        coeffs.append(rand_idx)\n",
    "    return coeffs\n",
    "\n",
    "coeffs_a = get_coefficients()\n",
    "coeffs_b = get_coefficients()\n",
    "    \n",
    "def get_minhash_signature(hashed_shingles):\n",
    "    signature = []\n",
    "    for i in range(signature_len):\n",
    "        min_hash_code = min([(coeffs_a[i] * shingle + coeffs_b[i]) % next_prime for shingle in hashed_shingles])\n",
    "        signature.append(min_hash_code)\n",
    "    return signature\n",
    "\n",
    "def minhash_docu(id, hashed_shingles):\n",
    "    return id, get_minhash_signature(hashed_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03dadeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_similarity(doc_1, doc_2, signature_len):\n",
    "    id_1, signature_1 = doc_1\n",
    "    id_2, signature_2 = doc_2\n",
    "    \n",
    "    agree_cnt = sum(1 for i in range(signature_len) if signature_1[i] == signature_2[i])\n",
    "    similarity = agree_cnt / signature_len\n",
    "    \n",
    "    return (id_1, id_2), similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2b1cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hash_rdd = shingles_rdd.map(lambda doc: minhash_docu(doc[0], doc[1]))\n",
    "minhash_pairs = min_hash_rdd.cartesian(min_hash_rdd).filter(lambda x: x[0][0] < x[1][0]) # remove duplicates\n",
    "minhash_with_similarities = minhash_pairs.map(lambda pair: signature_similarity(pair[0], pair[1], signature_len))\n",
    "minhash_threshold = minhash_with_similarities.filter(lambda x: x[1] >= similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d42b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                         (0 + 4) / 16]\r"
     ]
    }
   ],
   "source": [
    "result = minhash_with_similarities.collect()\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb85177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
